{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44597c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e09f1",
   "metadata": {},
   "source": [
    "### data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cube_data(L, N_ax, bc_prop=0.1):\n",
    "    # step size\n",
    "    d = L / N_ax\n",
    "\n",
    "    # generate xyz of cube (NOTE assumes bottom corner of cube sits at origin and extends into positive xyz) (NOTE not midpoints - includes boundaries)\n",
    "    x = np.linspace(0, L, N_ax)\n",
    "    y = np.linspace(0, L, N_ax)\n",
    "    z = np.linspace(0, L, N_ax)\n",
    "\n",
    "    # stack xyz into one array\n",
    "    cube = np.stack(np.meshgrid(x, y, z, indexing='ij'), axis=-1).reshape(-1, 3) \n",
    "\n",
    "    # split collocation and boundary data\n",
    "    coll_data = cube[np.all((cube != 0) & (cube != L), axis=1)]\n",
    "    bc_data = cube[np.any((cube == 0) | (cube == L), axis=1)]\n",
    "\n",
    "    # over/under sample coll data to fit desired bc_prop\n",
    "    coll_target_N = int(bc_data.shape[0] / bc_prop) - bc_data.shape[0]\n",
    "    if (coll_data.shape[0] < coll_target_N):\n",
    "        # oversample\n",
    "        remaining_N = coll_target_N - coll_data.shape[0]\n",
    "        np.random.shuffle(coll_data)\n",
    "        oversampled_coll = coll_data[np.random.choice([i for i, _ in enumerate(coll_data)], size=remaining_N)]  # TODO perturb these? might be fine since they get perturbed in training anyways\n",
    "        coll_data = np.concatenate([coll_data, oversampled_coll])\n",
    "    elif (coll_data.shape[0] > coll_target_N):\n",
    "        # undersample\n",
    "        np.random.shuffle(coll_data)\n",
    "        coll_data = coll_data[:coll_target_N]\n",
    "\n",
    "    # bc solution values\n",
    "    bc_u = np.array([0.] * bc_data.shape[0]).reshape(-1, 1)\n",
    "\n",
    "    print(f\"initial full cube shape: {cube.shape}\")\n",
    "    print(f\"step size: {d}\")\n",
    "    print(f\"collocation data shape: {coll_data.shape}\")\n",
    "    print(f\"boundary condition data shape: {bc_data.shape}\")\n",
    "    print(f\"total N after over/under sample: {coll_data.shape[0] + bc_data.shape[0]}\")\n",
    "\n",
    "    return coll_data, bc_data, bc_u, d\n",
    "\n",
    "def perturb(vals, minimum, maximum, perturb_delta):\n",
    "    # add noise to vals\n",
    "    noise = torch.randn_like(vals) * perturb_delta\n",
    "    new_vals = vals + noise\n",
    "    \n",
    "    # if perturbed vals fall outside of vals domain, move them back in\n",
    "    new_vals.data[new_vals < minimum] = minimum - new_vals.data[new_vals < minimum]  \n",
    "    new_vals.data[new_vals > maximum] = 2 * maximum - new_vals.data[new_vals > maximum]  \n",
    "\n",
    "    return new_vals\n",
    "\n",
    "def perturb_data(data, perturb_delta=0.01):\n",
    "    new_data = torch.ones_like(data)\n",
    "    minimums = [torch.min(data[:, i]).item() for i in range(data.shape[1])]\n",
    "    maximums = [torch.max(data[:, i]).item() for i in range(data.shape[1])]\n",
    "    perturb_deltas = [(torch.max(data[:, i]) - torch.min(data[:, i])).item() * perturb_delta for i in range(data.shape[1])]\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        new_data[:, i] = perturb(data[:, i], minimums[i], maximums[i], perturb_delta=perturb_deltas[i])\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def visualize(xyz_data):\n",
    "    # 3D xyz plot\n",
    "    xyzd = xyz_data\n",
    "    fig = plt.figure(figsize=(8, 6), constrained_layout=True)\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    scatter_ax = ax.scatter(xyzd[:, 0], xyzd[:, 1], xyzd[:, 2], c=[1. for _ in range(xyzd.shape[0])], cmap=\"Reds\")\n",
    "    colorbar = fig.colorbar(scatter_ax, shrink=0.55, aspect=6, pad=0.1)\n",
    "    colorbar.remove()\n",
    "    ax.set_xlabel(\"x\", weight=\"bold\")\n",
    "    ax.set_ylabel(\"y\", weight=\"bold\")\n",
    "    ax.set_zlabel(\"z\", weight=\"bold\")\n",
    "    ax.zaxis.labelpad = 5\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284ad13",
   "metadata": {},
   "source": [
    "### generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# generate grid of data\n",
    "L = 1\n",
    "N_ax = 25\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "coll_data, bc_data, bc_u, d = gen_cube_data(L, N_ax)\n",
    "visualize(coll_data)\n",
    "\n",
    "# separate train data\n",
    "train_N_ax = 10\n",
    "train_coll_data, train_bc_data, train_bc_u, train_d = gen_cube_data(L, train_N_ax)\n",
    "visualize(train_coll_data)\n",
    "visualize(train_bc_data)\n",
    "\n",
    "# cast arrays to tensors\n",
    "coll_data = torch.tensor(coll_data, requires_grad=True).to(device)\n",
    "bc_data = torch.tensor(bc_data, requires_grad=True).to(device)\n",
    "bc_u = torch.tensor(bc_u, requires_grad=True).to(device)\n",
    "\n",
    "train_coll_data = torch.tensor(train_coll_data, requires_grad=True).to(device)\n",
    "train_bc_data = torch.tensor(train_bc_data, requires_grad=True).to(device)\n",
    "train_bc_u = torch.tensor(train_bc_u, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed4a86",
   "metadata": {},
   "source": [
    "### loss class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ad2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINNLoss, self).__init__()\n",
    "        self.loss = None\n",
    "\n",
    "        # NOTE if you change loss terms returned, must change this and return+calculations in calc_loss()\n",
    "        self.return_names = [\"loss_ovr\", \"loss_ovr_no_reg\", \"loss_f\", \"loss_bc\", \"u_trivial_penalty\"]\n",
    "    \n",
    "    def MSE_f(self, f_pred):\n",
    "        \"\"\"\n",
    "        Returns MSE loss on f predictions. \n",
    "\n",
    "        Inputs:\n",
    "        f_pred: batch of differential equation predictions\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.mean(f_pred**2)\n",
    "\n",
    "    def MSE_bc(self, u_target, u_pred):\n",
    "        \"\"\"\n",
    "        Returns MSE loss on b.c. predictions.\n",
    "\n",
    "        Inputs:\n",
    "        u_target: actual value of u\n",
    "        u_pred: batch of predicted u values at b.c. points\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.mean((u_target - u_pred)**2)\n",
    "\n",
    "    def calc_u_trivial_penalty(self, u_pred):\n",
    "        \"\"\"\n",
    "        Returns a term that penalizes trivial solutions.\n",
    "\n",
    "        Inputs:\n",
    "        u_pred: batch of wavefunction predictions\n",
    "        \"\"\"\n",
    "\n",
    "        return (1 / torch.mean(u_pred**2 + 1e-6))\n",
    "\n",
    "    def calc_loss(self, u_coll_pred, f_pred, u_bc_target, u_bc_pred):\n",
    "        \"\"\"\n",
    "        Calculate loss terms for predictions. Returns a tuple of losses. \n",
    "        Names and order of returned terms should match self.return_names.\n",
    "\n",
    "        Inputs:\n",
    "        u_coll_pred: batch of wavefunction predictions \n",
    "        f_pred: batch of differential equation predictions (only generated from collocation data)\n",
    "        u_bc_target: actual u values for b.c. data\n",
    "        u_bc_pred: batch of b.c. wavefunction predictions\n",
    "        \"\"\"\n",
    "\n",
    "        # differential equation MSE loss (collocation)\n",
    "        loss_f = self.MSE_f(f_pred)\n",
    "\n",
    "        # trivial solution penalty (collocation)\n",
    "        u_trivial_penalty = self.calc_u_trivial_penalty(u_coll_pred)\n",
    "\n",
    "        # b.c. MSE loss\n",
    "        loss_bc = self.MSE_bc(u_bc_target, u_bc_pred)\n",
    "\n",
    "        # overall loss\n",
    "        loss_ovr = loss_f + loss_bc + u_trivial_penalty \n",
    "\n",
    "        # loss w/o regularization terms\n",
    "        loss_ovr_no_reg = loss_f + loss_bc\n",
    "\n",
    "        return loss_ovr, loss_ovr_no_reg, loss_f, loss_bc, u_trivial_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f79bb",
   "metadata": {},
   "source": [
    "### PINN class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinActivation(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        \"\"\"\n",
    "        Passes input tensor through sin function. Used in neural network class.\n",
    "\n",
    "        Inputs:\n",
    "        input: tensor batch of layer outputs\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.sin(input)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_size, device, E=None, num_hidden_layers=4, optimizer_lr=0.0001, optimizer_betas=(0.999, 0.9999)):\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        # constants\n",
    "        self.planck_term_constant = 0.5  # hbar**2/2*m in Coulomb units\n",
    "\n",
    "        # torch device\n",
    "        self.device = device\n",
    "\n",
    "        # E initialization\n",
    "        self.E = E \n",
    "\n",
    "        # network\n",
    "        self.activation = SinActivation()\n",
    "        self.dense0 = nn.Linear(input_size + 1, 64, dtype=torch.double)  # NOTE +1 to input shape for implicit E value\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(64, 64, dtype=torch.double) for _ in range(num_hidden_layers)])  # NOTE new\n",
    "        self.dense_out = nn.Linear(64, 1, dtype=torch.double)\n",
    "\n",
    "        # optimizer + loss\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=optimizer_lr, betas=optimizer_betas)\n",
    "        self.loss_criteria = PINNLoss()\n",
    "        self.loss = None\n",
    "\n",
    "    def forward_net(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass through full network. Returns network output.\n",
    "\n",
    "        Inputs:\n",
    "        data: tensor with correct input shape\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.dense0(data)\n",
    "        out = self.activation(out)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            out = hidden_layer(out)\n",
    "            out = self.activation(out)\n",
    "        out = self.dense_out(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"\n",
    "        Full PINN forward pass that includes E concat, network forward pass, network output b.c. scaling, \n",
    "        gradient calculations, and diff. eqn. (f) calculation.\n",
    "        Returns final output prediction, u, and f.\n",
    "\n",
    "        Inputs:\n",
    "        x: tensor of x with shape (N, 1)\n",
    "        y: tensor of y with shape (N, 1)\n",
    "        z: tensor of z with shape (N, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # create E input vector\n",
    "        E_batch = torch.tensor([[self.E]] * x.shape[0]).to(self.device)\n",
    "\n",
    "        # concat inputs for network (need them separate initially for grad calc later)\n",
    "        nn_inp = torch.cat([x, y, z, E_batch], dim=1)\n",
    "\n",
    "        # pass through nn\n",
    "        u = self.forward_net(nn_inp)\n",
    "\n",
    "        # calc gradients\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        u_z = torch.autograd.grad(u, z, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        u_zz = torch.autograd.grad(u_z, z, grad_outputs=torch.ones_like(u_z), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        # differential equation\n",
    "        f = self.planck_term_constant * (u_xx + u_yy + u_zz) + (self.E * u)\n",
    "        \n",
    "        return u, f\n",
    "\n",
    "    def forward_inference(self, x, y, z):\n",
    "        \"\"\"\n",
    "        Inference forward pass. Doesn't include gradient + f calculation.\n",
    "\n",
    "        Inputs:\n",
    "        x: tensor of x with shape (N, 1)\n",
    "        y: tensor of y with shape (N, 1)\n",
    "        z: tensor of z with shape (N, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # create E input vector\n",
    "        E_batch = torch.tensor([[self.E]] * x.shape[0]).to(self.device)\n",
    "\n",
    "        # concat inputs for network \n",
    "        nn_inp = torch.cat([x, y, z, E_batch], dim=1)\n",
    "\n",
    "        # pass through nn\n",
    "        u = self.forward_net(nn_inp)\n",
    "\n",
    "        return u\n",
    "\n",
    "    def backward(self, u_coll_pred, f_pred, u_bc_target, u_bc_pred):\n",
    "        \"\"\"\n",
    "        Network backward pass. Returns tuple of loss values.\n",
    "        \n",
    "        Inputs:\n",
    "        u_coll_pred: batch of collocation wavefunction predictions \n",
    "        f_pred: batch of differential equation predictions\n",
    "        u_bc_target: actual u values for b.c. data\n",
    "        u_bc_pred: batch of b.c. wavefunction predictions\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate losses\n",
    "        losses = self.loss_criteria.calc_loss(u_coll_pred, f_pred, u_bc_target, u_bc_pred)\n",
    "        self.loss = losses[0]  # NOTE overall loss should always be first value returned in losses tuple\n",
    "\n",
    "        # backprop + update params\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return losses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b62ff4",
   "metadata": {},
   "source": [
    "### plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plot(x_vals, y_vals, x_label=\"x\", y_label=\"y\", line_color=\"b\", log_scale=False, data_label=None, title=None):\n",
    "    \"\"\"\n",
    "    Generate plot for x_vals and y_vals.\n",
    "\n",
    "    Inputs:\n",
    "    pretty self-explanatory\n",
    "    \"\"\"\n",
    "\n",
    "    if data_label:\n",
    "        plt.plot(x_vals, y_vals, color=line_color, label=data_label)\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(x_vals, y_vals, color=line_color)\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    if log_scale:\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6235a24",
   "metadata": {},
   "source": [
    "### train loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc13f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_loop(coll_data, \n",
    "                   bc_data,\n",
    "                   bc_u,\n",
    "                   device, \n",
    "                   E_range,\n",
    "                   num_train_steps_per_E,\n",
    "                   save_denom=1000,\n",
    "                   loss_criteria_idx=0,\n",
    "                   num_hidden_layers=4):\n",
    "    start_time = time.time()\n",
    "    print(f\"training network on {len(E_range)} values of E with {num_train_steps_per_E} train steps per E...\")\n",
    "    epoch = 0\n",
    "    epoch_print_denom = 1000\n",
    "\n",
    "    # initialize network\n",
    "    lr = 8e-3\n",
    "    betas = (0.999, 0.9999)\n",
    "    pinn = PINN(coll_data.shape[1], device, optimizer_lr=lr, optimizer_betas=betas, num_hidden_layers=num_hidden_layers)\n",
    "    pinn.to(device)\n",
    "    \n",
    "    # initialize copy of current network (used for tracking best model across training)\n",
    "    current_best_pinn = PINN(coll_data.shape[1], device, E=pinn.E, optimizer_lr=lr, optimizer_betas=betas, num_hidden_layers=num_hidden_layers).to(device)\n",
    "    current_best_pinn.load_state_dict(pinn.state_dict())\n",
    "\n",
    "    # best model save variables\n",
    "    min_save_epoch = 100\n",
    "    prev_pinns = []  # list to store best models \n",
    "    default_min_loss = 1000\n",
    "    min_loss = default_min_loss\n",
    "    current_best_epoch = -1\n",
    "\n",
    "    E_predictions = []  # track E by training step (epoch)\n",
    "    train_loss_vals = []  # track all loss values across training\n",
    "\n",
    "    for E_idx, E in enumerate(E_range):\n",
    "        pinn.E = E\n",
    "        \n",
    "        for train_step in range(num_train_steps_per_E):\n",
    "\n",
    "            # perturb data\n",
    "            coll_perturbed = perturb_data(coll_data).to(device)\n",
    "            #coll_perturbed = coll_data\n",
    "\n",
    "            # collocation forward pass\n",
    "            u_coll_pred, f_pred = pinn.forward(coll_perturbed[:, 0].reshape(-1, 1), \n",
    "                                               coll_perturbed[:, 1].reshape(-1, 1),\n",
    "                                               coll_perturbed[:, 2].reshape(-1, 1))\n",
    "\n",
    "            # b.c. forward pass\n",
    "            u_bc_pred = pinn.forward_inference(bc_data[:, 0].reshape(-1, 1),\n",
    "                                               bc_data[:, 1].reshape(-1, 1),\n",
    "                                               bc_data[:, 2].reshape(-1, 1))\n",
    "\n",
    "            # backward pass\n",
    "            losses = pinn.backward(u_coll_pred, f_pred, bc_u, u_bc_pred)\n",
    "            train_loss_vals.append([loss_val.item() for loss_val in losses])\n",
    "\n",
    "            # track E\n",
    "            E_predictions.append(E)\n",
    "            \n",
    "            ####\n",
    "            # check if new minimum overall loss attained across this window of epochs\n",
    "            if (epoch >= min_save_epoch) and (losses[loss_criteria_idx] < min_loss):\n",
    "                # update current best loss and epoch\n",
    "                min_loss = losses[loss_criteria_idx]\n",
    "                current_best_epoch = epoch\n",
    "                \n",
    "                # copy current model state \n",
    "                current_best_pinn = PINN(coll_data.shape[1], device, E=pinn.E, optimizer_lr=lr, optimizer_betas=betas, num_hidden_layers=num_hidden_layers).to(device)\n",
    "                current_best_pinn.load_state_dict(pinn.state_dict())\n",
    "\n",
    "            # save the best (min loss) model across this window of epochs\n",
    "            if (epoch >= min_save_epoch) and ((epoch + 1) % save_denom == 0):\n",
    "                min_loss = default_min_loss  # reset min loss to default (ideally next best model beats quickly) \n",
    "\n",
    "                # copy this epoch window's best model state and save (model, epoch) tuple to list\n",
    "                pinn_copy = PINN(coll_data.shape[1], device, E=current_best_pinn.E, optimizer_lr=lr, optimizer_betas=betas, num_hidden_layers=num_hidden_layers).to(device)\n",
    "                pinn_copy.load_state_dict(current_best_pinn.state_dict())\n",
    "                prev_pinns.append((pinn_copy, current_best_epoch))\n",
    "\n",
    "                # NOTE I don't reset current_best_pinn here. so if model in next window doesn't beat default min loss, best model from previous window is saved\n",
    "            ####\n",
    "\n",
    "            if ((epoch == 0) or ((epoch + 1) % epoch_print_denom == 0)):\n",
    "                print(\"\\n\" + \"=\" * 20 + f\" EPOCH {epoch} \" + \"=\" * 20)\n",
    "                print()\n",
    "                print(\"train losses:\")\n",
    "                for return_name, val in zip(pinn.loss_criteria.return_names, train_loss_vals[epoch]):\n",
    "                    print(f\"{return_name} = {val:.3e}\")\n",
    "                print()\n",
    "                print(f\"current E = {pinn.E.item()}\")\n",
    "                print(f\"elapsed model training time: {(time.time() - start_time) / 60 :.2f} minutes\")\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "    # plot losses\n",
    "    epoch_list = [e for e in range(epoch)]\n",
    "    for loss_idx, loss_name in enumerate(pinn.loss_criteria.return_names):\n",
    "        display_plot(epoch_list,\n",
    "                     torch.tensor(train_loss_vals)[:, loss_idx].detach().numpy(),\n",
    "                     x_label=\"epoch\",\n",
    "                     y_label=loss_name,\n",
    "                     log_scale=True)\n",
    "\n",
    "    # plot E\n",
    "    display_plot(epoch_list, E_predictions, x_label=\"epoch\", y_label=r\"$\\hat{E}$\")\n",
    "\n",
    "    return prev_pinns, train_loss_vals, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321115b",
   "metadata": {},
   "source": [
    "### train model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2efe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_E(nx, ny, nz):\n",
    "    # Coulomb units: hbar**2/2m * pi**2/L**2 == 0.5 * pi**2/L**2\n",
    "    return (0.5 * np.pi**2 / L**2) * (nx**2 + ny**2 + nz**2)\n",
    "\n",
    "nx_ny_nz = [(1, 1, 1), (2, 1, 1), (3, 1, 1), (2, 2, 2)]\n",
    "E_range = np.array([calc_E(nx, ny, nz) for nx, ny, nz in nx_ny_nz])  # Coulomb units\n",
    "E_delta = 0.1\n",
    "E_range = np.array([[E - E_delta, E, E + E_delta] for E in E_range]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd13ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps_per_E = 30000\n",
    "loss_criteria_idx = 1\n",
    "saved_pinns, train_loss_vals, num_epochs = run_train_loop(train_coll_data, \n",
    "                                                          train_bc_data,\n",
    "                                                          train_bc_u,\n",
    "                                                          device, \n",
    "                                                          E_range, \n",
    "                                                          num_train_steps_per_E, \n",
    "                                                          loss_criteria_idx=loss_criteria_idx, \n",
    "                                                          num_hidden_layers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8212b",
   "metadata": {},
   "source": [
    "### get best model(s) and evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pinns(min_loss_windows, pinns, train_loss_vals, loss_criteria_idx):\n",
    "    \"\"\"\n",
    "    Plots model with minimum loss (based on loss_criteria_idx) in each window of min_loss_windows.\n",
    "    Returns list of dictionaries containing the best models + relevant info.\n",
    "\n",
    "    Inputs:\n",
    "    min_loss_windows: list of tuples where each tuple is a range of epoch values to look for model with minimum loss\n",
    "    pinns: list of all saved torch models across training\n",
    "    train_loss_vals: list of tuples containing loss values for each training epoch\n",
    "    loss_criteria_idx: index of loss term to use for evaluating best model to save (see PINNLoss)\n",
    "    \"\"\"\n",
    "\n",
    "    ovr_losses = torch.tensor(train_loss_vals)[:, loss_criteria_idx].detach().numpy()\n",
    "    eigen_pinns = []  # list to save best models to (assuming each model saved below corresponds to unique eigenvalue)\n",
    "\n",
    "    for (start, stop) in min_loss_windows:\n",
    "        # find the epoch where the minimum overall loss is achieved\n",
    "        epoch_idx = np.argmin(ovr_losses[start:stop]) + start\n",
    "\n",
    "        if (len(pinns) == 0):\n",
    "            print(\"No models saved during training.\")\n",
    "        else:\n",
    "            # get the matching model (this should always hit i think as long as there is a match)\n",
    "            p_list = [(net, epoch_num) for (net, epoch_num) in pinns if epoch_num == epoch_idx]\n",
    "\n",
    "            if (len(p_list) == 0):\n",
    "                print(f\"No matching saved models found for epoch window [{start}, {stop}] with minimum epoch = {epoch_idx}. Check self-defined minimum loss windows.\")\n",
    "            else:\n",
    "                # matching model found\n",
    "                p = p_list[0]\n",
    "                \n",
    "                # save this predicted eigen solution\n",
    "                eigen_pinns.append({\"model\": p[0], \"epoch\": epoch_idx, \"losses\": train_loss_vals[epoch_idx]})\n",
    "    \n",
    "    return eigen_pinns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c838ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_windows = [[100 + (i*num_train_steps_per_E * 3), (i*num_train_steps_per_E + num_train_steps_per_E) * 3] for i in range(4)]  # NOTE hardcoding\n",
    "best_pinns = get_best_pinns(loss_windows, saved_pinns, train_loss_vals, loss_criteria_idx)\n",
    "best_pinns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_analytical(x, y, z, nx, ny, nz, L):\n",
    "    # cube analytical solution\n",
    "    return np.sqrt(8 / L**3) * np.sin(nx * np.pi * x / L) * np.sin(ny * np.pi * y / L) * np.sin(nz * np.pi * z / L)\n",
    "\n",
    "def calc_pred_norm_const(approx_domain_pred_vec, d):\n",
    "    # 3D prediction normalization constant\n",
    "    return 1. / torch.sqrt(d**3 * torch.sum(approx_domain_pred_vec**2))\n",
    "\n",
    "def make_3D_plot(x_vals, \n",
    "                 y_vals, \n",
    "                 z_vals, \n",
    "                 cmap_vals, \n",
    "                 cbar_label=\"\",\n",
    "                 xlabel=\"\",\n",
    "                 ylabel=\"\",\n",
    "                 zlabel=\"\",\n",
    "                 ):\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(8, 6), constrained_layout=True)\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    scatter_ax = ax.scatter(x_vals, y_vals, z_vals, c=cmap_vals, cmap=\"Reds\")\n",
    "    colorbar = fig.colorbar(scatter_ax, shrink=0.55, aspect=6, pad=0.1)\n",
    "    colorbar.set_label(cbar_label, labelpad=30)\n",
    "    colorbar.ax.yaxis.label.set_rotation(0)\n",
    "    ax.set_xlabel(xlabel, weight=\"bold\")\n",
    "    ax.set_ylabel(ylabel, weight=\"bold\")\n",
    "    ax.set_zlabel(zlabel, weight=\"bold\")\n",
    "    ax.zaxis.labelpad = 10\n",
    "    ax.set_box_aspect(aspect=None, zoom=0.9)  \n",
    "    plt.show()\n",
    "\n",
    "def eval_pinn(pinn_dict, coll_data, bc_data, nx, ny, nz):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"PINN saved on epoch {pinn_dict['epoch']}\")\n",
    "    print(f\"E = {pinn_dict['model'].E}\")\n",
    "    \n",
    "    for loss_val, loss_name in zip(pinn_dict[\"losses\"], pinn_dict[\"model\"].loss_criteria.return_names):\n",
    "        print(f\"{loss_name} = {loss_val}\")\n",
    "\n",
    "    # combine bc+coll data for inference\n",
    "    data = torch.concat([coll_data, bc_data])\n",
    "\n",
    "    # make predictions and normalize\n",
    "    pinn = pinn_dict[\"model\"]\n",
    "    u_pred = pinn.forward_inference(data[:, 0].reshape(-1, 1), data[:, 1].reshape(-1, 1), data[:, 2].reshape(-1, 1))\n",
    "    pred_norm_const = calc_pred_norm_const(u_pred, d)  \n",
    "    u_pred = u_pred * pred_norm_const\n",
    "    u_pred = u_pred.flatten().detach().cpu().numpy()\n",
    "    u_pred_prob_densities = u_pred**2\n",
    "    x = data[:, 0].detach().cpu().numpy()\n",
    "    y = data[:, 1].detach().cpu().numpy()\n",
    "    z = data[:, 2].detach().cpu().numpy()\n",
    "\n",
    "    # analytical solution (NOTE based on input nx ny nz, could display different degenerate solution)\n",
    "    u_analytical = calc_analytical(x, y, z, nx, ny, nz, L)\n",
    "    u_analytical_prob_densities = u_analytical**2\n",
    "\n",
    "    # sign flip\n",
    "    flip_val = np.round(np.mean(u_analytical / (u_pred + 1e-6)))\n",
    "\n",
    "    if int(flip_val) < 0:\n",
    "        flipped_sign = True\n",
    "        u_pred = -u_pred\n",
    "    else:\n",
    "        flipped_sign = False\n",
    "\n",
    "    # MSE calc\n",
    "    mse = np.mean((u_analytical - u_pred)**2)\n",
    "\n",
    "    print(f\"flip val = {flip_val}\")\n",
    "    print(f\"flipped sign = {flipped_sign}\")\n",
    "    print(f\"MSE = {mse}\")\n",
    "\n",
    "    # subset predictions to points with prob density values greater than this\n",
    "    prob_thresh = 0.3\n",
    "    pred_prob_thresh_idx = np.where(u_pred_prob_densities > prob_thresh)[0]\n",
    "    x_pred_sub = x[pred_prob_thresh_idx]\n",
    "    y_pred_sub = y[pred_prob_thresh_idx]\n",
    "    z_pred_sub = z[pred_prob_thresh_idx]\n",
    "    u_pred_prob_densities_sub = u_pred_prob_densities[pred_prob_thresh_idx]\n",
    "    \n",
    "    # analytical subset\n",
    "    prob_thresh = 0.3\n",
    "    analytical_prob_thresh_idx = np.where(u_analytical_prob_densities > prob_thresh)[0]\n",
    "    x_analytical_sub = x[analytical_prob_thresh_idx]\n",
    "    y_analytical_sub = y[analytical_prob_thresh_idx]\n",
    "    z_analytical_sub = z[analytical_prob_thresh_idx]\n",
    "    u_analytical_prob_densities_sub = u_analytical_prob_densities[analytical_prob_thresh_idx]\n",
    "\n",
    "    # predicted u\n",
    "    make_3D_plot(x_pred_sub, y_pred_sub, z_pred_sub, u_pred_prob_densities_sub, cbar_label=r\"$|\\hat{\\psi}(x,y,z)|^2$\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n",
    "\n",
    "    # analytical u\n",
    "    make_3D_plot(x_analytical_sub, y_analytical_sub, z_analytical_sub, u_analytical_prob_densities_sub, cbar_label=r\"$|\\psi(x,y,z)|^2$\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced88090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pinn_dict, nx_ny_nz_tuple in zip(best_pinns, nx_ny_nz):\n",
    "    eval_pinn(pinn_dict, coll_data, bc_data, nx_ny_nz_tuple[0], nx_ny_nz_tuple[1], nx_ny_nz_tuple[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d3550",
   "metadata": {},
   "source": [
    "### messy hack to generate output plots of other degenerate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(pinn_dict, coll_data, bc_data, nx, ny, nz):\n",
    "    # combine bc+coll data for inference\n",
    "    data = torch.concat([coll_data, bc_data])\n",
    "\n",
    "    x = data[:, 0].detach().cpu().numpy()\n",
    "    y = data[:, 1].detach().cpu().numpy()\n",
    "    z = data[:, 2].detach().cpu().numpy()\n",
    "\n",
    "    # analytical solution\n",
    "    u_analytical = calc_analytical(x, y, z, nx, ny, nz, L)\n",
    "    u_analytical_prob_densities = u_analytical**2\n",
    "\n",
    "    # analytical subset\n",
    "    prob_thresh = 0.3\n",
    "    analytical_prob_thresh_idx = np.where(u_analytical_prob_densities > prob_thresh)[0]\n",
    "    x_analytical_sub = x[analytical_prob_thresh_idx]\n",
    "    y_analytical_sub = y[analytical_prob_thresh_idx]\n",
    "    z_analytical_sub = z[analytical_prob_thresh_idx]\n",
    "    u_analytical_prob_densities_sub = u_analytical_prob_densities[analytical_prob_thresh_idx]\n",
    "\n",
    "    # analytical u\n",
    "    make_3D_plot(x_analytical_sub, y_analytical_sub, z_analytical_sub, u_analytical_prob_densities_sub, cbar_label=r\"$|\\psi(x,y,z)|^2$\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad265cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output(None, coll_data, bc_data, 1, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output(None, coll_data, bc_data, 1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d25ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINNs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
